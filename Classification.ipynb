{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Therese\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Therese\\Anaconda3\\envs\\py36\\lib\\site-packages\\ggplot\\utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "C:\\Users\\Therese\\Anaconda3\\envs\\py36\\lib\\site-packages\\ggplot\\stats\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\Therese\\Anaconda3\\envs\\py36\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ficlearn.feature_extraction.text import BnsTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from itertools import product\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from ficlearn.metrics import crossValidationScores\n",
    "import codecs as cs\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from ggplot import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_to_categories(df):\n",
    "    \n",
    "    price_list = df['price'].tolist()\n",
    "    \n",
    "    max_price = df['price'].max()\n",
    "    min_price = df['price'].min()\n",
    "    mean_price = df['price'].mean()\n",
    "    \n",
    "    cate = []\n",
    "    \n",
    "    for i, each in enumerate(price_list):\n",
    "        if each < 2500000:\n",
    "            cate.append(0)\n",
    "\n",
    "        \n",
    "        elif each > 4000000:\n",
    "            cate.append(2)\n",
    "            \n",
    "        else:\n",
    "            cate.append(1)\n",
    "    \n",
    "    column_values = pd.Series(cate)\n",
    "    df.insert(loc=0, column='categories', value=column_values)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_calc(X, n = 1):\n",
    "    tf = TfidfVectorizer(ngram_range=(n,n), min_df = 10)\n",
    "    \n",
    "    x = tf.fit_transform(X)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    stop = stopwords.words('swedish') + list(string.punctuation.encode('utf-8')) + ['gt', 'lt', 'amp', 'quot', 'align', '**', '***', '--', '//', '://', '),', ').']\n",
    "    for i, s in enumerate(stop):\n",
    "        stop[i] = str(s).replace(u'\\xe5', 'aa').replace(u'\\xe4', 'ae').replace(u'\\xf6', 'oe')\n",
    "    result = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = []\n",
    "        doc = row['description']\n",
    "        for word in nltk.wordpunct_tokenize(doc.lower()):\n",
    "            if word not in stop and not is_int(word):\n",
    "                sent.append(word)\n",
    "        sent = ' '.join(sent)\n",
    "        result.append(sent)\n",
    "    df['tokens'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_calc(vocab, train, test, n = 1):\n",
    "    tf = TfidfVectorizer(ngram_range=(n,n), min_df = 10)\n",
    "    counts = tf.fit(vocab['tokens'])\n",
    "    train_mtx = tf.transform(train).toarray()\n",
    "    test_mtx = tf.transform(test).toarray()\n",
    "    \n",
    "\n",
    "    return train_mtx, test_mtx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    vocab = pd.read_json('output_new.json')\n",
    "    df = pd.read_json('sthlm_format.json')\n",
    "    print('Processing data....')\n",
    "    preprocessing(vocab)\n",
    "    preprocessing(df)\n",
    "    Y = sort_to_categories(df)['categories']\n",
    "    X = df['tokens']\n",
    "    \n",
    "    return X,Y, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X,Y,vocab):\n",
    "    \n",
    "    \n",
    "    test_size = 0.30\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size = test_size, random_state = 0)\n",
    "    \n",
    "    \n",
    "    print('Calculating tfidf....')\n",
    "   \n",
    "    X_train, X_test = tfid_calc(vocab, X_train, X_test,1)\n",
    "    \n",
    "    print(\"Classifying....\")\n",
    "    \n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=4)\n",
    "    \n",
    "    kn = KNeighborsClassifier(n_neighbors=6)\n",
    "    \n",
    "    svm = SVC(kernel='linear', probability=True)\n",
    "    \n",
    "    gnb = MultinomialNB()\n",
    "    voting = VotingClassifier(estimators=[('dt', tree), ('knn', kn),\n",
    "                                         ('bayes', gnb)],\n",
    "                            voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "    print('tree')\n",
    "    y_pred_tree = tree.fit(X_train, y_train).predict(X_test)\n",
    "    print('kn')\n",
    "    y_pred_kn = kn.fit(X_train, y_train).predict(X_test)\n",
    "    #y_pred_svm = svm.fit(X_train, y_train).predict(X_test)\n",
    "    print('bayes')\n",
    "    y_pred_bayes = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    print('voting')\n",
    "    #y_pred_voting = voting.fit(X_train, y_train).predict(X_test)\n",
    "        \n",
    "    print(metrics.accuracy_score(y_test, y_pred_tree))\n",
    "    print(metrics.accuracy_score(y_test, y_pred_kn))\n",
    "    #print(metrics.accuracy_score(y_test, y_pred_svm))\n",
    "    print(metrics.accuracy_score(y_test, y_pred_bayes))\n",
    "    #print(metrics.accuracy_score(y_test, y_pred_voting))\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_test, y_pred_tree))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_kn))\n",
    "    #print(metrics.confusion_matrix(y_test, y_pred_svm))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_bayes))\n",
    "    #print(metrics.confusion_matrix(y_test, y_pred_voting))\n",
    "    \n",
    "    \n",
    "    return y_test, y_pred_bayes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data....\n"
     ]
    }
   ],
   "source": [
    "X,Y,vocab = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tfidf....\n",
      "Classifying....\n",
      "tree\n",
      "kn\n",
      "bayes\n",
      "voting\n",
      "0.439941046426\n",
      "0.523212969786\n",
      "0.610169491525\n",
      "0.567428150332\n",
      "[[107 296  25]\n",
      " [ 50 372  58]\n",
      " [  7 324 118]]\n",
      "[[224 195   9]\n",
      " [112 310  58]\n",
      " [ 57 216 176]]\n",
      "[[225 194   9]\n",
      " [ 70 356  54]\n",
      " [ 17 185 247]]\n",
      "[[177 237  14]\n",
      " [ 68 348  64]\n",
      " [ 13 191 245]]\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred_bayes = run(X,Y,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get A classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Low       0.72      0.53      0.61       428\n",
      "     Medium       0.48      0.74      0.59       480\n",
      "       High       0.80      0.55      0.65       449\n",
      "\n",
      "avg / total       0.66      0.61      0.61      1357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Low', 'Medium', 'High']\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_bayes, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate R2-Score for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data....\n"
     ]
    }
   ],
   "source": [
    "vocab = pd.read_json('output_new.json')\n",
    "df = pd.read_json('sthlm_format.json')\n",
    "print('Processing data....')\n",
    "preprocessing(vocab)\n",
    "preprocessing(df)\n",
    "df = sort_to_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby('categories').mean()['price'].tolist()\n",
    "medians = df.groupby('categories').median()['price'].tolist()\n",
    "y_pred = y_pred_bayes\n",
    "y_test1 = y_test.as_matrix()\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    np.place(y_pred, y_pred==i, means[i])\n",
    "    np.place(y_pred, y_pred==i, means[i])\n",
    "    np.place(y_pred, y_pred==i, means[i])\n",
    "    \n",
    "for i in range(3):\n",
    "    \n",
    "    np.place(y_test1, y_test1==i, means[i])\n",
    "    np.place(y_test1, y_test1==i, means[i])\n",
    "    np.place(y_test1, y_test1==i, means[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26784654045808698"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000000, 3195000, 5600000]\n",
      "[2014851.389878831, 3226690.8459214503, 6698648.409556314]\n"
     ]
    }
   ],
   "source": [
    "print(medians)\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For visualization of distribution of category samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = df.groupby('categories').size().plot(kind='bar', figsize=(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some play with the number of features, preserved Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tokens']\n",
    "X_tfidf = tfid_calc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "ncomps = 1000\n",
    "svd = TruncatedSVD(n_components=ncomps)\n",
    "svd_fit = svd.fit(X_tfidf)\n",
    "Y = svd.fit_transform(X_tfidf) \n",
    "ax = pd.Series(svd_fit.explained_variance_ratio_.cumsum()).plot(kind='line', figsize=(10,3))\n",
    "print('Variance preserved by first 1000 components == {:.2%}'.format(\n",
    "        svd_fit.explained_variance_ratio_.cumsum()[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualizatino of data 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = pd.DataFrame(Y, columns=['c{}'.format(c) for c in range(ncomps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plotdims = 5\n",
    "ploteorows = 1\n",
    "svdcols = [c for c in new_X.columns if c[0] == 'c']\n",
    "dfsvdplot = new_X[svdcols].iloc[:,:plotdims]\n",
    "dfsvdplot['class'] = df['class']\n",
    "ax = sns.pairplot(dfsvdplot.iloc[::ploteorows,:], hue='class', size=1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualisation 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_scatter(A, elevation=30, azimuth=120):\n",
    "    \"\"\" Create 3D scatterplot \"\"\"\n",
    "    \n",
    "    maxpts=1000\n",
    "    fig = plt.figure(1, figsize=(9, 9))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elevation, azim=azimuth)\n",
    "    ax.set_xlabel('component 0')\n",
    "    ax.set_ylabel('component 1')\n",
    "    ax.set_zlabel('component 2')\n",
    "\n",
    "    # plot subset of points\n",
    "    rndpts = np.sort(np.random.choice(A.shape[0], min(maxpts,A.shape[0]), replace=False))\n",
    "    coloridx = np.unique(A.iloc[rndpts]['class'], return_inverse=True)\n",
    "    colors = coloridx[1] / len(coloridx[0])   \n",
    "    \n",
    "    sp = ax.scatter(A.iloc[rndpts,0], A.iloc[rndpts,1], A.iloc[rndpts,2]\n",
    "               ,c=colors, cmap=\"jet\", marker='o', alpha=0.6\n",
    "               ,s=50, linewidths=0.8, edgecolor='#BBBBBB')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interactive, fixed\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plotdims = 1000\n",
    "\n",
    "svdcols = [c for c in new_X.columns if c[0] == 'c']\n",
    "dfsvd = new_X[svdcols].iloc[:,:plotdims]\n",
    "dfsvd['class'] = df['class']\n",
    "interactive(plot_3d_scatter, A=fixed(dfsvd), elevation=30, azimuth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X.shape, df['categories'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsvd['class'] = df['categories']\n",
    "df = dfsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Size of the dataframe: {}'.format(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndperm = np.random.permutation(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[svdcols].values)\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "df['pca-three'] = pca_result[:,2]\n",
    "\n",
    "'Explained variation per principal component: {}'.format(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chart = ggplot( df.loc[rndperm[:3000],:], aes(x='pca-one', y='pca-two', color='class') ) \\\n",
    "        + geom_point(size=25,alpha=0.8) \\\n",
    "        + ggtitle(\"First and Second Principal Components colored by digit\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_50 = PCA(n_components=600)\n",
    "pca_result_50 = pca_50.fit_transform(df[svdcols].values)\n",
    "\n",
    "'Explained variation per principal component (PCA): {}'.format(np.sum(pca_50.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "n_sne = 5000\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result_50[rndperm[:n_sne]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = None\n",
    "df_tsne = df.loc[rndperm[:n_sne],:].copy()\n",
    "df_tsne['x-tsne-pca'] = tsne_pca_results[:,0]\n",
    "df_tsne['y-tsne-pca'] = tsne_pca_results[:,1]\n",
    "\n",
    "chart = ggplot( df_tsne, aes(x='x-tsne-pca', y='y-tsne-pca', color='class') ) \\\n",
    "        + geom_point(size=20,alpha=0.1) \\\n",
    "        + ggtitle(\"tSNE dimensions colored by Digit (PCA)\")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
