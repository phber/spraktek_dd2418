{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from ficlearn.feature_extraction.text import BnsTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from itertools import product\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from ficlearn.metrics import crossValidationScores\n",
    "import codecs as cs\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_to_categories(df, lower=2500000, higher=4000000):\n",
    "\n",
    "    #In: dataframe with all data points, lower and higher decision bounderies\n",
    "    #Out: Dataframe with all data points sorted into respective category\n",
    "    #Effect: Sorting data points into three categories, boundery determine by inout parameters lower and higher.\n",
    "    \n",
    "    price_list = df['price'].tolist()\n",
    "    \n",
    "    max_price = df['price'].max()\n",
    "    min_price = df['price'].min()\n",
    "    mean_price = df['price'].mean()\n",
    "    \n",
    "    categories = []\n",
    "    \n",
    "    for i, each in enumerate(price_list):\n",
    "        if each < lower:\n",
    "            categories.append(0)\n",
    "\n",
    "        \n",
    "        elif each > higher:\n",
    "            categories.append(2)\n",
    "            \n",
    "        else:\n",
    "            categories.append(1)\n",
    "    \n",
    "    column_values = pd.Series(categories)\n",
    "    df.insert(loc=0, column='categories', value=column_values)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    #In: Dataframe that should be processed\n",
    "    #Out: Tokenized dataframe. \n",
    "    #Effect: Removal of stopwords (Swedish most common words), non-letters, åäö is replaces with aa, ae, oe.\n",
    "    \n",
    "    stop = stopwords.words('swedish') + list(string.punctuation.encode('utf-8')) + ['gt', 'lt', 'amp', 'quot', 'align', '**', '***', '--', '//', '://', '),', ').']\n",
    "    for i, s in enumerate(stop):\n",
    "        stop[i] = str(s).replace(u'\\xe5', 'aa').replace(u'\\xe4', 'ae').replace(u'\\xf6', 'oe')\n",
    "    result = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = []\n",
    "        doc = row['description']\n",
    "        for word in nltk.wordpunct_tokenize(doc.lower()):\n",
    "            if word not in stop and not is_int(word):\n",
    "                sent.append(word)\n",
    "        sent = ' '.join(sent)\n",
    "        result.append(sent)\n",
    "    df['tokens'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(s):\n",
    "    #In: token\n",
    "    #Effect: Check if token is integer, if so return True. If not return False.\n",
    "    \n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfid_calc(vocab, train, test, n = 1):\n",
    "    #In: Vocabulary of words in corpus, train and test data set and which n-gram model.\n",
    "    #Out: Vectorized form of train and test data set\n",
    "    #Effect: Vectorize text from the train and test sets\n",
    "    \n",
    "    tf = TfidfVectorizer(ngram_range=(n,n), min_df = 10)\n",
    "    counts = tf.fit(vocab['tokens'])\n",
    "    train_mtx = tf.transform(train).toarray()\n",
    "    test_mtx = tf.transform(test).toarray()\n",
    "    \n",
    "    return train_mtx, test_mtx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #Out: Y - Labels for all data points in data set, X - vectoized texts for each data points in the data set\n",
    "    #     vocab - Vocuabulary of all words in the corpus\n",
    "    #Effect: Reads in file of all listings in sweden to create a larger vocabulary. Reads in listings limited to the stockholm region.\n",
    "    #        Preprocess the data frames. Creating labels for each datapoint.\n",
    "    \n",
    "    vocab = pd.read_json('output_new.json')\n",
    "    df = pd.read_json('sthlm_format.json')\n",
    "    print('Processing data....')\n",
    "    preprocessing(vocab)\n",
    "    preprocessing(df)\n",
    "    Y = sort_to_categories(df)['categories']\n",
    "    X = df['tokens']\n",
    "    \n",
    "    return X,Y, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X,Y,vocab):\n",
    "    \n",
    "    test_size = 0.30\n",
    "    print('Splitting into test and training sets')\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size = test_size)\n",
    "        \n",
    "    print('Calculating tfidf....')\n",
    "   \n",
    "    X_train, X_test = tfid_calc(vocab, X_train, X_test)\n",
    "    \n",
    "    print(\"Classifying....\")\n",
    "    \n",
    "    gnb = MultinomialNB()\n",
    "\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    return [y_test, y_pred]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results):\n",
    "    #In: Result from the classification, true and predicted labels of the test data set. \n",
    "    #Effect: Calculating and prints average metrics over all iterations of the classifier\n",
    "    \n",
    "    #Number of iterations of the classifier\n",
    "    N = len(results)\n",
    "    \n",
    "    #Initilizing matricies for the confusion matrix and the recall,precision and f-score matrices.\n",
    "    list_of_confusion = [np.array([]) for i in range(N)]\n",
    "    list_of_report = [np.array([]) for i in range(N)]\n",
    "\n",
    "    \n",
    "    #Calculate confusion matricies, prcision, recall anf fscore of each result\n",
    "    for i,each in enumerate(results):\n",
    "\n",
    "        list_of_confusion[i] = np.array(metrics.confusion_matrix(each[0], each[1]))\n",
    "\n",
    "        list_of_report[i] =metrics.precision_recall_fscore_support(each[0], each[1])\n",
    "    \n",
    "    #Calculate mean and standard deviaion of instances of the confusion matrix\n",
    "    Number_of_categoires = 3\n",
    "    t=4\n",
    "    range1 = lambda start, end: range(start, end+1) # modified range function\n",
    "    mean_conf = np.mean([list_of_confusion[t-j] for j in range1(1,Number_of_categoires)], axis = 0)\n",
    "    std_conf = np.std([list_of_confusion[t-j] for j in range1(1,Number_of_categoires)], axis = 0)\n",
    "\n",
    "    #Initilizing precision, recall and f1-score matrices\n",
    "    prec = [list_of_report[i][0] for i in range(0,N)]\n",
    "    recalls = [list_of_report[i][1] for i in range(0,N)]\n",
    "    f1scores = [list_of_report[i][2] for i in range(0,N)]\n",
    "    \n",
    "    \n",
    "    #Calculating mean for precision, recall and f1 score\n",
    "    prec_mean = np.mean([[list_of_report[i][0] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    recall_mean = np.mean([[list_of_report[i][1] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    f1_mean = np.mean([[list_of_report[i][2] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    \n",
    "    #Calculating Standard Deviation for precision, recall and f1 score\n",
    "    prec_sd = np.std([[list_of_report[i][0] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    recall_sd = np.std([[list_of_report[i][1] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    f1_sd = np.std([[list_of_report[i][2] for i in range(0,N)][j] for j in range(0,Number_of_categoires)], axis = 0)\n",
    "    \n",
    "    print('Mean confusion Matrix')\n",
    "    print(mean_conf)\n",
    "    \n",
    "    print('Confusion Matrix, standard deviation of each instance of the matrix')\n",
    "    print(std_conf)\n",
    "    \n",
    "    print('Mean precision for each class:' + prec_mean)\n",
    "    print('Mean Recall for each class:' + recall_mean)\n",
    "    print('Mean F1-score for each class:' + f1_mean)\n",
    "    \n",
    "    print('St dev of precision for each class:' + prec_sd)\n",
    "    print('St dev of Recall for each class:' + recall_sd)\n",
    "    print('St dev of F1-score for each class:' + f1_sd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nr_iter=100):\n",
    "    #In: NUmber of iterations of classification\n",
    "    #Out: Results of the classification\n",
    "    \n",
    "    X,Y,vocab = read_data()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(nr_iter):\n",
    "        results.append(classify(X,Y,vocab))\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main(nr_iter) function with number of wished classificatin iterations (default=100). The result is the predcted and real labels for the test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69911339  0.49007476  0.76606571]\n",
      "[ 0.54438058  0.69540131  0.56954089]\n",
      "[ 0.61114828  0.57440066  0.65257248]\n",
      "[ 0.01611135  0.02222723  0.01987316]\n",
      "[ 0.03246322  0.01536031  0.02494182]\n",
      "[ 0.0158282   0.01229015  0.00874642]\n"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print average metrics of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
